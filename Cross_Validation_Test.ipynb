{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation \n",
    "def compute_mse_loss(y, tx, w):\n",
    "    \"\"\" return mse loss \"\"\"\n",
    "    e = y - np.dot(tx,w)\n",
    "    return 1/2 * np.mean(e**2)\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    grad = -(np.dot(tx.T,e))/N\n",
    "    return grad\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"sigmoid function \"\"\"\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_lg_loss(y, x, w):\n",
    "    e = 1e-11\n",
    "    p = sigmoid(np.dot(x, w))\n",
    "    loss = -np.dot(y, np.log(p + e)) - np.dot((1 - y).T, np.log(1 - p + e))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_lg_grad(y, x, w):\n",
    "    p = sigmoid(np.dot(x, w))\n",
    "    grad = np.dot(x.T, (p - y))\n",
    "    return grad\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    threshold = 1e-8\n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        grad = grad/np.linalg.norm(grad)\n",
    "\n",
    "        loss = compute_mse_loss(y, tx, w)\n",
    "\n",
    "        w = w - gamma*grad\n",
    "        # store loss in order to check the convergence\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1]-losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    return w,losses\n",
    "\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    threshold = 1e-8\n",
    "    batch_size = 1\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "\n",
    "        random_num = np.random.randint(0, len(y), size=batch_size)\n",
    "        r_y = y[random_num]\n",
    "        r_tx = tx[random_num]\n",
    "\n",
    "        grad = compute_gradient(r_y, r_tx, w)\n",
    "        grad = grad/np.linalg.norm(grad)\n",
    "\n",
    "        w = w - gamma*grad\n",
    "\n",
    "        # store loss in order to check the convergence\n",
    "        loss = compute_mse_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1]-losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    return w,losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    XTX = np.dot(tx.T,tx)\n",
    "    XTY = np.dot(tx.T,y)\n",
    "    w = np.linalg.solve(XTX, XTY)\n",
    "    loss = compute_mse_loss(y, tx, w)\n",
    "\n",
    "    return w,loss\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "\n",
    "    reg = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    XTX = np.dot(tx.T,tx) + reg\n",
    "    XTY = np.dot(tx.T,y)\n",
    "    w = np.linalg.solve(XTX, XTY)\n",
    "    loss = compute_mse_loss(y, tx, w)\n",
    "\n",
    "    return w,loss\n",
    "\n",
    "# Logistic regression\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    losses = []\n",
    "    threshold = 1e-8\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        grad = compute_lg_grad(y, tx, w)\n",
    "        grad = grad / np.linalg.norm(grad)\n",
    "\n",
    "        w = w - gamma * grad\n",
    "        # store loss in order to check the convergence\n",
    "        loss = compute_lg_loss(y, tx, w)\n",
    "        # print(loss)\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1]-losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    return w, losses\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_ , initial_w, max_iters, gamma):\n",
    "\n",
    "\n",
    "    w = initial_w\n",
    "    losses = []\n",
    "    threshold = 1e-8\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        grad = compute_lg_grad(y, tx, w)\n",
    "        grad =  grad + 2*lambda_*w\n",
    "\n",
    "        grad = grad / np.linalg.norm(grad)\n",
    "\n",
    "        w = w - gamma * grad\n",
    "\n",
    "        # store loss in order to check the convergence\n",
    "        loss = compute_lg_loss(y, tx, w)\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# divide data into three groups\n",
    "def divide_data(tx,y):\n",
    "\n",
    "    grp_1_tx = tx[tx[:, 22] >= 2]\n",
    "    def_1 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29]\n",
    "    grp_1_tx = grp_1_tx[:, def_1]\n",
    "    \n",
    "    grp_2_tx = tx[tx[:, 22] == 1]\n",
    "    def_2 = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29]\n",
    "    grp_2_tx = grp_2_tx[:, def_2]\n",
    "\n",
    "    grp_3_tx = tx[tx[:, 22] == 0]\n",
    "    def_3 = [0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "    grp_3_tx = grp_3_tx[:, def_3]\n",
    "\n",
    "    grp_1_y = y[tx[:, 22] >= 2]\n",
    "    grp_2_y = y[tx[:, 22] == 1]\n",
    "    grp_3_y = y[tx[:, 22] == 0]\n",
    "\n",
    "    return grp_1_tx, grp_2_tx, grp_3_tx, grp_1_y, grp_2_y, grp_3_y, def_1, def_2, def_3\n",
    "\n",
    "# normalize data\n",
    "def normalize(tX):\n",
    "    for i in range(tX.shape[1]):\n",
    "        if (tX[:,i] == 1).all(): #it's not necessary to normalize the bais term\n",
    "            continue\n",
    "        tX[:, i] = (tX[:, i] - min(tX[:, i])) / (max(tX[:, i] - min(tX[:, i])))\n",
    "\n",
    "    return tX\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "def build_tx_poly(grp_tX, degree1):\n",
    "\n",
    "    if grp_tX.shape[1] == 30:\n",
    "        grp_tX_poly_der = build_poly(grp_tX[:,:13], degree1)\n",
    "        grp_tX_poly_pri = grp_tX[:,13:]\n",
    "        grp_tX_poly = np.c_[grp_tX_poly_der, grp_tX_poly_pri]\n",
    "    else:\n",
    "        grp_tX_poly_der = build_poly(grp_tX[:,:10], degree1)\n",
    "        grp_tX_poly_pri = grp_tX[:,10:]\n",
    "        grp_tX_poly = np.c_[grp_tX_poly_der, grp_tX_poly_pri]\n",
    "    return grp_tX_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, k):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # generate random indices\n",
    "\n",
    "    data_size = len(y)\n",
    "    indices = np.random.permutation(data_size)\n",
    "    cros_val_set = []\n",
    "    label_set = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # ratio = i/k\n",
    "        fold = x[indices[int(i / k * data_size):int((i + 1) / k * data_size)]]\n",
    "        label = y[indices[int(i / k * data_size):int((i + 1) / k * data_size)]]\n",
    "        cros_val_set.append(fold)\n",
    "        label_set.append(label)\n",
    "\n",
    "    return cros_val_set, label_set\n",
    "\n",
    "def build_combinations(k):\n",
    "\n",
    "    folds_id = set()\n",
    "    leave_one_out = set()\n",
    "    combinations = []\n",
    "    for i in range(k):\n",
    "        folds_id.add(i)\n",
    "    for i in range(k):\n",
    "        leave_one_out.add(i)\n",
    "        combinations.append(folds_id.difference(leave_one_out))\n",
    "        leave_one_out = set()\n",
    "\n",
    "    return combinations,folds_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outlier(tX):\n",
    "    for i in range(tX.shape[1]):\n",
    "        # calculate summary statistics\n",
    "        data = tX[:,i]\n",
    "        data_mean, data_std = np.mean(data), np.std(data)\n",
    "        # identify outliers\n",
    "        cut_off = data_std * 3\n",
    "        lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "        outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "        value_rep_lo = np.min(outliers_removed)\n",
    "        value_rep_up = np.max(outliers_removed)\n",
    "        idx_outlier_lo = np.where((data < lower))\n",
    "        idx_outlier_up = np.where((data > upper))\n",
    "        data[idx_outlier_lo] = value_rep_lo\n",
    "        data[idx_outlier_up] = value_rep_up\n",
    "        tX[:,i] = data\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "from proj1_helpers import *\n",
    "# TODO: download train data and supply path here \n",
    "DATA_TRAIN_PATH = '/Users/eddie/Desktop/Machine Learning CS-433/ML_course/projects/project1/dataset/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX[tX[:,0]==-999] = np.median(tX[tX[:,0]!=-999,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_1_tx, grp_2_tx, grp_3_tx, grp_1_y, grp_2_y, grp_3_y, def_1, def_2, def_3 = divide_data(tX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training data has been devided into three groups \n",
    "# and before testing each group by algorithm defined above,\n",
    "# we process the raw data by cleaning the outliers, \n",
    "# adding polynomial basis and normalization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_1_tx = clean_outlier(grp_1_tx)\n",
    "grp_2_tx = clean_outlier(grp_2_tx)\n",
    "grp_3_tx = clean_outlier(grp_3_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "degree1 = 9\n",
    "degree2 = 9\n",
    "degree3 = 9\n",
    "\n",
    "grp_1_tx_poly = build_tx_poly(grp_1_tx, degree1)\n",
    "grp_2_tx_poly = build_tx_poly(grp_2_tx, degree2)\n",
    "grp_3_tx_poly = build_tx_poly(grp_3_tx, degree3)\n",
    "\n",
    "grp_1_tx_poly = normalize(grp_1_tx_poly)\n",
    "grp_2_tx_poly = normalize(grp_2_tx_poly)\n",
    "grp_3_tx_poly = normalize(grp_3_tx_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for logistic regression and regularized logistic regression\n",
    "# grp_1_y[grp_1_y==-1] = 0\n",
    "# grp_2_y[grp_2_y==-1] = 0\n",
    "# grp_3_y[grp_3_y==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can decide which data group we would like to test based on cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cros_val_set, label_set = split_data(grp_1_tx_poly, grp_1_y, 5)\n",
    "# cros_val_set, label_set = split_data(grp_2_tx_poly, grp_2_y, 5)\n",
    "# cros_val_set, label_set = split_data(grp_3_tx_poly, grp_3_y, 5)\n",
    "\n",
    "combinations,folds_id = build_combinations(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(cros_val_set, label_set, combinations,folds_id):\n",
    "\n",
    "    loss = 0;\n",
    "    count = 0;\n",
    "    num = 0;\n",
    "    for combination in combinations:\n",
    "        tr = []\n",
    "        tr_l = []\n",
    "        te_id = list(folds_id.difference(combination))[0]\n",
    "        te = cros_val_set[te_id]\n",
    "        te_l = label_set[te_id]\n",
    "        for fold_id in combination:\n",
    "            tr.append(cros_val_set[fold_id])\n",
    "            tr_l.append(label_set[fold_id])\n",
    "        tr = np.vstack(tr)\n",
    "        tr_l = np.hstack(tr_l)\n",
    "        \n",
    "        #los is used to check the convergence \n",
    "        #w, los = least_squares_SGD(tr_l, tr, initial_w, 1800, 0.03)\n",
    "        #w, los = least_squares_GD(tr_l, tr, initial_w, 500, 0.03)\n",
    "        #w, los = least_squares(tr_l, tr)\n",
    "        #w, los = ridge_regression(tr_l, tr, 0.00000000001)\n",
    "        #w, los = logistic_regression(tr_l, tr, np.zeros(grp_2_tx_poly.shape[1]), 10000, 0.1)\n",
    "        #w, los = reg_logistic_regression(tr_l, tr, 0.000000001 , np.zeros(grp_3_tx_poly.shape[1]), 10000, 0.02)\n",
    "        \n",
    "        pred_l = predict_labels(w, te)\n",
    "        \n",
    "        #only for logistic regression and regularized logistic regression\n",
    "        #pred_l [pred_l==-1] =0\n",
    "        \n",
    "        loss += len(te_l) - sum(te_l == pred_l)\n",
    "        count += sum(te_l == pred_l)\n",
    "    \n",
    "        num += len(te_l)\n",
    "    loss = loss / len(folds_id) #this loss is the number of wrong predicitions\n",
    "    acc = count / num\n",
    "    return loss, acc, los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by selecting different methods and parameters in the validate function, \n",
    "#we can test their performance on different data group separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(grp_3_tx_poly.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,acc,los = validate(cros_val_set, label_set, combinations,folds_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_learn]",
   "language": "python",
   "name": "conda-env-pytorch_learn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
